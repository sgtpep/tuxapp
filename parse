#!/usr/bin/env python
from __future__ import print_function
import functools
import glob
import imp
import os
import re
import sys

try:
  from html.parser import HTMLParser
except ImportError:
  from HTMLParser import HTMLParser

tuxapp = imp.load_source('tuxapp', os.path.join(os.path.dirname(__file__), 'tuxapp'))

class BaseParser(HTMLParser):
  content = ''
  content_attributes = {}
  content_tag = None
  is_head = False
  is_multiple = False
  previous_attributes = {}
  previous_tag = None
  results = []
  schema_org_type = None

  data_types = (
    'Product',
    'SoftwareApplication',
    'WebSite',
  )

  def __init__(self, *args, **kwargs):
    HTMLParser.__init__(self, *args, **kwargs)
    self.results = []

  def add_result(self, result):
    self.results.append(self.filter_result(result) if self.is_multiple else (result[:-1], self.filter_result(result[-1])))

  def feed(self, *args, **kwargs):
    HTMLParser.feed(self, *args, **kwargs)
    self.raise_result()

  def filter_result(self, string):
    return re.sub(r"\s+", ' ', string.replace('\t', ' ')).strip()

  def handle_data(self, data):
    getattr(self, 'on_data', lambda *args, **kwargs: None)(self.previous_tag, self.previous_attributes, data.strip())
    if self.content_tag:
      self.content += data
    if self.previous_attributes.get('itemprop'):
      getattr(self, 'on_schema_org', lambda *args, **kwargs: None)(self.schema_org_type, data, self.previous_tag, self.previous_attributes)
    if self.previous_tag == 'script' and self.previous_attributes.get('type') == 'application/ld+json' and data.strip():
      import json
      decoded_data = json.loads(data)
      items = decoded_data if isinstance(decoded_data, list) else [decoded_data]
      for item in items:
        if '@type' in item:
          getattr(self, 'on_json_ld', lambda *args, **kwargs: None)(item)

  def handle_endtag(self, tag):
    if tag == self.content_tag:
      getattr(self, 'on_content', lambda *args, **kwargs: None)(self.content_tag, self.content_attributes, self.content.strip())
      self.content = ''
      self.content_attributes = {}
      self.content_tag = None
    if self.is_head and tag == 'head':
      self.raise_result()

  def handle_starttag(self, tag, attributes):
    attributes = dict(attributes)
    getattr(self, 'on_tag', lambda *args, **kwargs: None)(tag, attributes)
    if attributes.get('itemtype'):
      self.schema_org_type = attributes['itemtype'].rsplit('/', 1)[-1]
    if tag == 'meta' and attributes.get('content') and attributes.get('itemprop'):
      getattr(self, 'on_schema_org', lambda *args, **kwargs: None)(self.schema_org_type, attributes['content'], tag, attributes)
    self.previous_attributes = attributes
    self.previous_tag = tag

  def raise_result(self):
    if self.is_multiple:
      raise ResultException(self.results)
    else:
      raise ResultException(sorted(self.results, key=lambda item: item[:-1], reverse=True)[0][-1] if self.results else '')

  def read_content(self, tag, attributes):
    self.content_attributes = attributes
    self.content_tag = tag

class BaseURLParser(BaseParser):
  def on_tag(self, tag, attributes):
    if tag == 'base' and attributes.get('href'):
      raise ResultException(attributes['href'])

class ChangelogURLParser(BaseParser):
  priority = (
    'anchor',
    'schema-org',
    'json-ld',
  )

  def on_content(self, tag, attributes, content):
    if tag == 'a' and attributes.get('href') and re.match(r"(changelog|release\s+notes|what[^\s]+s\s+new)\b", content, re.I | re.S):
      self.add_result((self.priority.index('anchor'), attributes['href']))

  def on_json_ld(self, item):
    if item.get('releaseNotes'):
      self.add_result((self.priority.index('json-ld'), item['releaseNotes']))

  def on_schema_org(self, type, content, tag, attributes):
    if attributes['itemprop'] == 'releaseNotes':
      self.add_result((self.priority.index('schema-org'), content))

  def on_tag(self, tag, attributes):
    if tag == 'a':
      self.read_content(tag, attributes)

class DownloadsURLParser(BaseParser):
  priority = (
    'download',
    'downloads',
  )

  def filter_url(self, url):
    return re.sub(r"^.+\b(github\.com/[\w-]+/[\w-]+/releases)\b.*$", r"https://\1", url, 1)

  def on_content(self, tag, attributes, content):
    if tag == 'a' and attributes.get('href'):
      if re.match(r"(desktop\s+apps|downloads?)$", content, re.I | re.S) or re.search(r"\s+(downloads|platforms)$", content, re.I | re.S):
        self.add_result((self.priority.index('downloads'), self.filter_url(attributes['href'])))
      elif re.match(r"[Dd]ownload\s+[A-Z]", content, re.S) and not content.endswith(" ZIP"):
        self.add_result((self.priority.index('download'), self.filter_url(attributes['href'])))

  def on_tag(self, tag, attributes):
    if tag == 'a':
      self.read_content(tag, attributes)

class GitHubScreenshotURLsParser(BaseParser):
  is_article = False
  is_multiple = True
  paragraph_number = 0

  def on_tag(self, tag, attributes):
    if tag == 'article':
      self.is_article = True
    elif tag == 'p' and self.is_article:
      self.paragraph_number += 1
    elif tag == 'img' and attributes.get('src') and self.previous_tag == 'a' and is_image_url(self.previous_attributes.get('href', '')) and self.is_article and self.paragraph_number > 1:
      self.add_result(attributes['src'])

class ImageURLParser(BaseParser):
  priority = (
    'og-image',
    'schema-org',
    'json-ld',
  )

  def on_json_ld(self, item):
    if item['@type'] in self.data_types and item.get('image'):
      self.add_result((self.priority.index('json-ld'), item['image']))

  def on_schema_org(self, type, content, tag, attributes):
    if type in self.data_types and attributes['itemprop'] == 'image':
      self.add_result((self.priority.index('schema-org'), content))

  def on_tag(self, tag, attributes):
    if tag == 'meta' and attributes.get('property') in ('og:image', 'og:image:url', 'og:image:secure-url') and attributes.get('content'):
      self.add_result((self.priority.index('og-image'), attributes['content']))

class ResultException(Exception):
  pass

class ScreenshotURLsParser(BaseParser):
  is_multiple = True
  tag = None

  def on_tag(self, tag, attributes):
    if tag in ('a', 'img') and re.search(r"\bscreen", str(attributes), re.I):
      if tag == 'a' and attributes.get('href') and is_image_url(attributes['href']) and "/File:" not in attributes['href'] and (not self.tag or self.tag == tag):
        self.tag = tag
        self.add_result(attributes['href'])
      elif tag == 'img' and (not self.tag or self.tag == tag):
        self.tag = tag
        if attributes.get('srcset'):
          self.add_result(attributes['srcset'].split()[0])
        elif attributes.get('src'):
          self.add_result(attributes['src'])

class ScreenshotsURLParser(BaseParser):
  def on_content(self, tag, attributes, content):
    if tag == 'a' and attributes.get('href') and re.search(r"\bscreenshots\b", content, re.I):
      raise ResultException(attributes['href'])

  def on_tag(self, tag, attributes):
    if tag == 'a':
      self.read_content(tag, attributes)

class VideoPosterURLsParser(BaseParser):
  is_multiple = True

  def on_tag(self, tag, attributes):
    if (tag == 'source' and self.previous_tag == 'video' or tag == 'video') and attributes.get('src') and tuxapp.parse_url(attributes['src']).path.endswith(".mp4"):
      self.add_result((attributes if tag == 'video' else self.previous_attributes).get('poster', ''))

class VideoURLsParser(BaseParser):
  is_multiple = True

  def on_tag(self, tag, attributes):
    if (tag == 'source' and self.previous_tag == 'video' or tag == 'video') and attributes.get('src') and tuxapp.parse_url(attributes['src']).path.endswith(".mp4"):
      self.add_result(attributes['src'])

def parse_html(parser, html):
  try:
    parser().feed(html)
  except ResultException as exception:
    return exception.args[0] if exception.args else ''

def update_app_info_worker(app):
  try:
    return update_app_info(app)
  except:
    import traceback
    traceback.print_exc()
    raise

def update_apps_info(apps):
  import contextlib
  import multiprocessing
  with contextlib.closing(multiprocessing.Pool(10)) as pool:
    try:
      functools.reduce(lambda result, app_result: result and app_result, pool.imap_unordered(update_app_info_worker, apps), True)
    except AssertionError as exception:
      if exception.args:
        print(*exception.args, file=sys.stderr)
  return True

are_app_debian_screenshots_ignored = lambda app: \
  app in (
    'skype',
  )

are_app_github_screenshots_ignored = lambda app: \
  app in (
    'jumpfm',
  )

check_github_repository = lambda repository, format="{}": \
  format.format(repository) \
    if repository and not re.search(r"/releases$", fetch_headers_cached("https://github.com/{}/releases/latest".format(repository)), re.M) else \
  ''

extract_github_repository = lambda url: \
  tuxapp.search(r"\bgithub\.com/([\w-]+/[\w-]+)/releases\b", fetch_url_cached(url), 0, 1) or \
  tuxapp.search(r"\bgithub\.com/([\w-]+/[\w-]+)\b", fetch_url_cached(url), 0, 1) or \
  ''

fetch_headers_cached = tuxapp.memoizes()(
  lambda url: tuxapp.fetch_headers(url)
)

fetch_url_cached = tuxapp.memoizes()(
  lambda url: tuxapp.fetch_url(url)
)

filter_changelog_url = lambda url, changelog_url: \
  '' if re.search({
    "https://www.mozilla.org/en-US/firefox/developer/": r"/firefox/",
  }.get(url, r"^$"), changelog_url) else changelog_url

filter_downloads_url = lambda url, downloads_url: \
  '' if re.search({
    "https://www.google.com/chrome/browser/beta.html": r"/chrome/browser/$",
    "https://www.mozilla.org/en-US/firefox/developer/": r"/firefox/",
    "https://www.mozilla.org/en-US/thunderbird/": r"/firefox/",
  }.get(url, r"^$"), downloads_url) else downloads_url

filter_unique = lambda urls, reference_urls=None: \
  (zip(*functools.reduce(
    lambda items, item: items + (() if item in items else (item,)),
    zip(urls, reference_urls or urls),
    (),
  )) or [()])[0]

filter_url = lambda url: re.sub(r"\.pagespeed\.ce\..+$", '', url)

get_app_icon_path = lambda app, url: "{}/icon-{}".format(get_app_path(app), url if url == '*' else tuxapp.hash_md5(url))

get_app_image_path = lambda app, url: "{}/image-{}".format(get_app_path(app), url if url == '*' else tuxapp.hash_md5(url))

get_app_info_path = lambda app: "{}/info".format(get_app_path(app))

get_app_path = lambda app: "{}/tuxapp-parse/{}".format(tuxapp.get_xdg_cache_path(), app)

get_app_screenshot_path = lambda app, url: "{}/screenshot-{}".format(get_app_path(app), url if url == '*' else tuxapp.hash_md5(url))

get_app_video_thumbnail_path = lambda app, url: "{}/video-{}".format(get_app_path(app), url if url == '*' else tuxapp.hash_md5(url))

get_apps = lambda arguments: (tuxapp.extract_app(argument) for argument in arguments or glob.iglob(tuxapp.get_appfile_path('*')))

is_image_url = lambda url: tuxapp.parse_url(url).path.rsplit('.', 1)[-1].lower() in ('gif', 'jpeg', 'jpg', 'png', 'svg')

is_screenshot_url_ignored = lambda url, screenshot_url: \
  bool(re.search({
    "https://code.visualstudio.com/": r"\.svg$",
  }.get(url, r"^$"), screenshot_url))

main = \
  tuxapp.handles_exceptions(
  tuxapp.silences(
    lambda arguments=tuple(sys.argv[1:]): \
      update_apps_info(get_apps(arguments)) and \
      functools.reduce(lambda result, app: result and update_all_app_files(app), get_apps(arguments), True)
  ))

normalize_url = lambda page_url, url, base_url=None: \
  url \
    if re.match(r"https?://", url) else \
  "{}:{}".format(tuxapp.parse_url(page_url).scheme, url) \
    if url.startswith("//") else \
  "{0.scheme}://{0.netloc}{1}".format(tuxapp.parse_url(page_url), url) \
    if url.startswith('/') else \
  "{0.scheme}://{0.netloc}{1}{2}".format(
    tuxapp.parse_url(base_url or parse_base_url(page_url)),
    os.path.normpath(os.path.join(os.path.dirname(tuxapp.parse_url(base_url or parse_base_url(page_url)).path or '/'), url)),
    '/' if url and url[-1] == '/' else '',
  ) \
    if url and url != '#' and not re.match(r"\w+:", url) else \
  ''

parse_app_debian_screenshot_urls = lambda app: \
  () \
    if are_app_debian_screenshots_ignored(app) else \
  tuple(url for url in parse_screenshot_urls("https://screenshots.debian.net/package/{}".format(app)) if not url.endswith("/no-screenshots-available.svg"))

parse_app_github_screenshot_urls = lambda app, url: \
  () \
    if are_app_github_screenshots_ignored(app) else \
  tuple(normalize_url(url, screenshot_url) for screenshot_url in parse_html(GitHubScreenshotURLsParser, fetch_url_cached(url)))

parse_app_screenshot_urls = lambda app, url: \
  parse_app_github_screenshot_urls(app, url) \
    if re.search(r"\bgithub\.com/[\w-]+/[\w-]+$", url) else \
  parse_screenshot_urls(parse_screenshots_url(url) or url) + \
  (parse_app_github_screenshot_urls(app, "https://github.com/{}".format(extract_github_repository(url))) if extract_github_repository(url) else ()) + \
  parse_app_debian_screenshot_urls(app)

parse_base_url = lambda url: normalize_url(url, parse_html(BaseURLParser, fetch_url_cached(url)) or url, url)

parse_changelog_url = lambda url: \
  filter_changelog_url(url,
    normalize_url(url, parse_html(ChangelogURLParser, fetch_url_cached(url))) or \
    normalize_url(url, parse_html(ChangelogURLParser, fetch_url_cached(normalize_url(url, parse_html(DownloadsURLParser, fetch_url_cached(url))) or url))) or \
    parse_github_releases_url(url)
  )

parse_download_formats = lambda url: \
  ' '.join(sorted(set(format.lower() for format in re.findall(r"\b(appimage|deb|flatpak|rpm|snap|tar)\b", fetch_url_cached(
    parse_github_latest_release_url(url) or \
    parse_downloads_url(url) or \
    url
  ), re.I))))

parse_downloads_url = lambda url: \
  filter_downloads_url(url,
    normalize_url(url, parse_html(DownloadsURLParser, fetch_url_cached(url))) or \
    parse_github_latest_release_url(url)
  )

parse_github_latest_release_url = lambda url: check_github_repository(extract_github_repository(url), "https://github.com/{}/releases/latest")

parse_github_releases_url = lambda url: check_github_repository(extract_github_repository(url), "https://github.com/{}/releases")

parse_github_repository = lambda url: check_github_repository(extract_github_repository(url))

parse_image_url = lambda url: \
  '' \
    if re.search(r"\bgithub\.com/[\w-]+/[\w-]+$", url) else \
  filter_url(normalize_url(url, parse_html(ImageURLParser, fetch_url_cached(url))))

parse_screenshot_urls = lambda url: filter(bool, tuple(filter_url(normalize_url(url, screenshot_url)) for screenshot_url in parse_html(ScreenshotURLsParser, fetch_url_cached(url)) if not is_screenshot_url_ignored(url, screenshot_url)))

parse_screenshots_url = lambda url: normalize_url(url, parse_html(ScreenshotsURLParser, fetch_url_cached(url)))

parse_video_poster_urls = lambda url: filter(bool, tuple(filter_url(normalize_url(url, poster_url)) or '-' for video_url, poster_url in zip(parse_html(VideoURLsParser, fetch_url_cached(url)), parse_html(VideoPosterURLsParser, fetch_url_cached(url)))))

parse_video_src_urls = lambda url: filter(bool, tuple(filter_url(normalize_url(url, video_url)) for video_url in parse_html(VideoURLsParser, fetch_url_cached(url))))

parse_video_thumbnail_urls = lambda url: parse_youtube_thumbnail_urls(url) + parse_video_poster_urls(url)

parse_video_urls = lambda url: parse_youtube_urls(url) + parse_video_src_urls(url)

parse_youtube_ids = lambda url: tuple(set(re.findall(r"\b(?:youtu\.be/|youtube\.com/(?:(?:embed|v)/|watch\?v=))(\w+)", fetch_url_cached(url))))

parse_youtube_thumbnail_urls = lambda url: tuple("https://img.youtube.com/vi/{}/hqdefault.jpg".format(id) for id in parse_youtube_ids(url))

parse_youtube_urls = lambda url: tuple("https://www.youtube.com/watch?v={}".format(id) for id in parse_youtube_ids(url))

query_app_info = \
  tuxapp.asserts(lambda app, key, *args, **kwargs: kwargs['result'] is None and "Failed to get {}.{}".format(app, key))(
  tuxapp.asserts(lambda app, *args, **kwargs: kwargs['result'] is None and not os.path.isfile(get_app_info_path(app)) and "No parsed info for {}".format(app))(
    lambda app, key: tuxapp.search(r"(?<=^{}=).*$".format(re.escape(key)), tuxapp.read_file(get_app_info_path(app)), re.M)
  ))

update_all_app_files = lambda app: \
  update_app_file(app, get_app_icon_path, tuxapp.query_appfile(app, 'icon-url')) and \
  update_app_file(app, get_app_image_path, query_app_info(app, 'image-url')) and \
  update_app_files(app, get_app_screenshot_path, query_app_info(app, 'screenshot-urls').split()) and \
  update_app_files(app, get_app_video_thumbnail_path, tuple(url for url in query_app_info(app, 'video-thumbnail-urls').split() if url != '-'))

update_app = lambda app: update_app_info(app) and update_all_app_files(app)

update_app_file = lambda app, get_path, url: update_app_files(app, get_path, (url,) if url else ())

update_app_files = lambda app, get_path, urls: \
  all(tuxapp.remove_file(path) for path in glob.iglob(get_path(app, '*')) if path not in tuple(get_path(app, url) for url in urls)) and \
  all(tuxapp.download_missing_file(url, get_path(app, url)) for url in urls)

update_app_info = lambda app: \
  write_app_info(app, {
    'changelog-url': parse_changelog_url(tuxapp.query_appfile(app, 'homepage-url')),
    'download-formats': parse_download_formats(tuxapp.query_appfile(app, 'homepage-url')),
    'downloads-url': parse_downloads_url(tuxapp.query_appfile(app, 'homepage-url')),
    'github-repository': parse_github_repository(tuxapp.query_appfile(app, 'homepage-url')),
    'image-url': parse_image_url(tuxapp.query_appfile(app, 'homepage-url')),
    'screenshot-urls': ' '.join(filter_unique(parse_app_screenshot_urls(app, tuxapp.query_appfile(app, 'homepage-url')))),
    'video-thumbnail-urls': ' '.join(filter_unique(parse_video_thumbnail_urls(tuxapp.query_appfile(app, 'homepage-url')), parse_video_urls(tuxapp.query_appfile(app, 'homepage-url')))),
    'video-urls': ' '.join(filter_unique(parse_video_urls(tuxapp.query_appfile(app, 'homepage-url')))),
  })

write_app_info = lambda app, info: tuxapp.write_file(get_app_info_path(app), ''.join("{}={}\n".format(key, info[key]) for key in sorted(info)))

if __name__ == '__main__':
  main()
